{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd235fb0",
   "metadata": {},
   "source": [
    "# Bias Buccaneers Image Recognition Challenge: Quickstart\n",
    "\n",
    "This notebook will introduce you to the data and describe a workflow to train and evaluate a baseline model on it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad57b0b7",
   "metadata": {},
   "source": [
    "## Initial Setup\n",
    "\n",
    "We start with loading the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "2454f20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "from pathlib import Path\n",
    "from functools import lru_cache\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "import cv2\n",
    "import imgaug.augmenters as iaa\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as K\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "\n",
    "logger = logging.basicConfig(\n",
    "    level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536f1fd6",
   "metadata": {},
   "source": [
    "## Load the Data\n",
    "\n",
    "Make sure to download and uncompress the data (`data_bb1_img_recognition.zip`) in the folder you're working off of.\n",
    "\n",
    "We first load the file containing the labels, binarize labels of each of the three classes as a numpy array and store them as a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "0c742f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "LOADPATH = Path(\"../Downloads/data_bb1/train/\")\n",
    "assert LOADPATH.exists()\n",
    "SAVEPATH = './models/'\n",
    "df = pd.read_csv( \"train/labels.csv\")\n",
    "aug_df = pd.read_csv(LOADPATH / \"aug_labels.csv\")\n",
    "new_aug = aug_df[[\"aug_name\", \"skin_tone\", \"gender\", \"age\"]].rename(columns={\"aug_name\": \"name\"})\n",
    "df_labeled = pd.concat([df, new_aug], axis=0).dropna()\n",
    "\n",
    "# Converting labels to np array\n",
    "cat = ['skin_tone','gender','age']\n",
    "lbs = [LabelBinarizer() for i in range(3)]\n",
    "Y = []\n",
    "for i in range(3):\n",
    "    lab = lbs[i].fit_transform(df_labeled[cat[i]])\n",
    "    if lab.shape[1]==1:\n",
    "        Y.append(np.hstack((1-lab,lab)))\n",
    "    else:\n",
    "        Y.append(lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "b2dfcbdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading images\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Could not import PIL.Image. The use of `load_img` requires PIL.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\jhr\\face_recognition_bounty\\quickstart_jhr.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jhr/face_recognition_bounty/quickstart_jhr.ipynb#X45sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m length \u001b[39m=\u001b[39m width \u001b[39m=\u001b[39m \u001b[39m64\u001b[39m \u001b[39m# size for each input image, increase if you want\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jhr/face_recognition_bounty/quickstart_jhr.ipynb#X45sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m nn \u001b[39m=\u001b[39m df_labeled\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/jhr/face_recognition_bounty/quickstart_jhr.ipynb#X45sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m all_imgs \u001b[39m=\u001b[39m [image\u001b[39m.\u001b[39mload_img(LOADPATH\u001b[39m/\u001b[39mdf_labeled\u001b[39m.\u001b[39miloc[i][\u001b[39m'\u001b[39m\u001b[39mname\u001b[39m\u001b[39m'\u001b[39m], target_size\u001b[39m=\u001b[39m(length,width)) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(nn)]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jhr/face_recognition_bounty/quickstart_jhr.ipynb#X45sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mConverting images to np array\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jhr/face_recognition_bounty/quickstart_jhr.ipynb#X45sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m X \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mempty([nn, length, width, \u001b[39m3\u001b[39m], dtype\u001b[39m=\u001b[39m\u001b[39mfloat\u001b[39m)\n",
      "\u001b[1;32mc:\\Users\\jhr\\face_recognition_bounty\\quickstart_jhr.ipynb Cell 6\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jhr/face_recognition_bounty/quickstart_jhr.ipynb#X45sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m length \u001b[39m=\u001b[39m width \u001b[39m=\u001b[39m \u001b[39m64\u001b[39m \u001b[39m# size for each input image, increase if you want\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jhr/face_recognition_bounty/quickstart_jhr.ipynb#X45sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m nn \u001b[39m=\u001b[39m df_labeled\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/jhr/face_recognition_bounty/quickstart_jhr.ipynb#X45sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m all_imgs \u001b[39m=\u001b[39m [image\u001b[39m.\u001b[39;49mload_img(LOADPATH\u001b[39m/\u001b[39;49mdf_labeled\u001b[39m.\u001b[39;49miloc[i][\u001b[39m'\u001b[39;49m\u001b[39mname\u001b[39;49m\u001b[39m'\u001b[39;49m], target_size\u001b[39m=\u001b[39;49m(length,width)) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(nn)]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jhr/face_recognition_bounty/quickstart_jhr.ipynb#X45sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mConverting images to np array\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jhr/face_recognition_bounty/quickstart_jhr.ipynb#X45sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m X \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mempty([nn, length, width, \u001b[39m3\u001b[39m], dtype\u001b[39m=\u001b[39m\u001b[39mfloat\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\jhr\\Anaconda3\\envs\\cds-vis\\lib\\site-packages\\keras\\preprocessing\\image.py:313\u001b[0m, in \u001b[0;36mload_img\u001b[1;34m(path, grayscale, color_mode, target_size, interpolation)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[39m@keras_export\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mkeras.utils.load_img\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m    278\u001b[0m               \u001b[39m'\u001b[39m\u001b[39mkeras.preprocessing.image.load_img\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    279\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_img\u001b[39m(path, grayscale\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, color_mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrgb\u001b[39m\u001b[39m'\u001b[39m, target_size\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    280\u001b[0m              interpolation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mnearest\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m    281\u001b[0m   \u001b[39m\"\"\"Loads an image into PIL format.\u001b[39;00m\n\u001b[0;32m    282\u001b[0m \n\u001b[0;32m    283\u001b[0m \u001b[39m  Usage:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    311\u001b[0m \u001b[39m      ValueError: if interpolation method is not supported.\u001b[39;00m\n\u001b[0;32m    312\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 313\u001b[0m   \u001b[39mreturn\u001b[39;00m image\u001b[39m.\u001b[39;49mload_img(path, grayscale\u001b[39m=\u001b[39;49mgrayscale, color_mode\u001b[39m=\u001b[39;49mcolor_mode,\n\u001b[0;32m    314\u001b[0m                         target_size\u001b[39m=\u001b[39;49mtarget_size, interpolation\u001b[39m=\u001b[39;49minterpolation)\n",
      "File \u001b[1;32mc:\\Users\\jhr\\Anaconda3\\envs\\cds-vis\\lib\\site-packages\\keras_preprocessing\\image\\utils.py:111\u001b[0m, in \u001b[0;36mload_img\u001b[1;34m(path, grayscale, color_mode, target_size, interpolation)\u001b[0m\n\u001b[0;32m    109\u001b[0m     color_mode \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mgrayscale\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    110\u001b[0m \u001b[39mif\u001b[39;00m pil_image \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 111\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mCould not import PIL.Image. \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    112\u001b[0m                       \u001b[39m'\u001b[39m\u001b[39mThe use of `load_img` requires PIL.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    113\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(path, \u001b[39m'\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m    114\u001b[0m     img \u001b[39m=\u001b[39m pil_image\u001b[39m.\u001b[39mopen(io\u001b[39m.\u001b[39mBytesIO(f\u001b[39m.\u001b[39mread()))\n",
      "\u001b[1;31mImportError\u001b[0m: Could not import PIL.Image. The use of `load_img` requires PIL."
     ]
    }
   ],
   "source": [
    "\n",
    "# loading and converting data into np array\n",
    "print(\"Loading images\")\n",
    "length = width = 64 # size for each input image, increase if you want\n",
    "nn = df_labeled.shape[0]\n",
    "all_imgs = [image.load_img(LOADPATH/df_labeled.iloc[i]['name'], target_size=(length,width)) for i in range(nn)]\n",
    "\n",
    "print(\"Converting images to np array\")\n",
    "X = np.empty([nn, length, width, 3], dtype=float)\n",
    "for i in range(nn):\n",
    "    X[i,:] = image.img_to_array(all_imgs[i])\n",
    "X = K.applications.resnet50.preprocess_input(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95116c58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "monk_3     1700\n",
      "monk_4     1552\n",
      "monk_5     1285\n",
      "monk_2     1283\n",
      "monk_7      694\n",
      "monk_6      632\n",
      "monk_1      515\n",
      "monk_8      485\n",
      "monk_9      289\n",
      "monk_10     118\n",
      "Name: skin_tone, dtype: int64\n",
      "female    5106\n",
      "male      3447\n",
      "Name: gender, dtype: int64\n",
      "18_30     4250\n",
      "31_60     2126\n",
      "0_17      1901\n",
      "61_100     276\n",
      "Name: age, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "for col in cat:\n",
    "    print(df[col].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb73277",
   "metadata": {},
   "source": [
    "We then load the images under the training set and convert them to numpy arrays. This may take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0f957a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@lru_cache(maxsize=1000)\n",
    "def load_rgb(path: Path, rgb=False) -> np.ndarray:\n",
    "    img = cv2.imread(str(path))\n",
    "    if not rgb:\n",
    "        return img\n",
    "    return cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "def show_img(img):\n",
    "    cv2.imshow('image',img)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "testimg = load_rgb(LOADPATH / df_labeled.loc[1, \"name\"])\n",
    "\n",
    "show_img(testimg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f0693f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = iaa.Sequential([\n",
    "    iaa.Crop(px=(1, 16), keep_size=False),\n",
    "    iaa.Fliplr(0.5),\n",
    "    iaa.GaussianBlur(sigma=(0, 2.0))\n",
    "])\n",
    "aug_img = seq.augment_image(testimg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e7d6bdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_aug_names(names: pd.Series) -> pd.Series:\n",
    "    rand_ints = pd.Series(np.random.randint(10000000, 99999999, size=names.shape[0]), index=names.index).astype(\"string\")\n",
    "    return rand_ints + names.str.replace(\"TRAIN\", \"_AUG\")\n",
    "\n",
    "def get_imgs_to_sample(series: pd.Series) -> pd.Series:\n",
    "    counts = series.value_counts()\n",
    "    return counts.max() - counts\n",
    "\n",
    "age_counts = df_labeled[\"age\"].value_counts()\n",
    "img_to_sample = age_counts.max() - age_counts\n",
    "\n",
    "\n",
    "def create_augs(df_sample: pd.DataFrame, img_to_sample: pd.Series, seq: iaa.Sequential, colname: str = \"age\") -> pd.DataFrame:\n",
    "    new_df = pd.DataFrame()\n",
    "    for cat, count in img_to_sample.items():\n",
    "        logging.info(f\"Sampling {count} images for {colname} {cat}\")\n",
    "        if count > 0:\n",
    "            df_sample = df_labeled[df_labeled[colname] == cat].sample(count, replace=True)\n",
    "            df_sample[\"aug_name\"] = create_aug_names(df_sample[\"name\"])\n",
    "            sample_imgs = {name: load_rgb(LOADPATH / name) for name in df_sample[\"name\"]}\n",
    "            augs = seq.augment_images(list(sample_imgs.values()))\n",
    "            for i, aug in enumerate(augs):\n",
    "                cv2.imwrite(str(LOADPATH / df_sample.iloc[i, 4]), aug)\n",
    "            new_df = pd.concat([new_df, df_sample])\n",
    "    return new_df\n",
    "\n",
    "#new_df = create_augs(df_labeled, img_to_sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "df6b4094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taken from: https://www.kaggle.com/code/andreagarritano/simple-data-augmentation-with-imgaug/notebook\n",
    "newseq = iaa.Sequential([\n",
    "    iaa.Fliplr(0.5),\n",
    "    iaa.Crop(percent=(0, 0.1)),\n",
    "    iaa.Sometimes(0.5,\n",
    "        iaa.GaussianBlur(sigma=(0, 0.5))\n",
    "    ),\n",
    "    iaa.LinearContrast((0.75, 1.5)),\n",
    "    iaa.AdditiveGaussianNoise(loc=0, scale=(0.0, 0.05*255), per_channel=0.5),\n",
    "    iaa.Multiply((0.8, 1.2), per_channel=0.2),\n",
    "\n",
    "], random_order=True)\n",
    "\n",
    "testy = seq.augment_image(testimg)\n",
    "show_img(testy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "c78b5046",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>skin_tone</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TRAIN0001.png</td>\n",
       "      <td>monk_1</td>\n",
       "      <td>female</td>\n",
       "      <td>18_30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRAIN0002.png</td>\n",
       "      <td>monk_6</td>\n",
       "      <td>male</td>\n",
       "      <td>0_17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TRAIN0004.png</td>\n",
       "      <td>monk_3</td>\n",
       "      <td>male</td>\n",
       "      <td>0_17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>TRAIN0005.png</td>\n",
       "      <td>monk_2</td>\n",
       "      <td>male</td>\n",
       "      <td>0_17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>TRAIN0006.png</td>\n",
       "      <td>monk_2</td>\n",
       "      <td>female</td>\n",
       "      <td>18_30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12276</th>\n",
       "      <td>TRAIN9993.png</td>\n",
       "      <td>monk_2</td>\n",
       "      <td>male</td>\n",
       "      <td>18_30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12278</th>\n",
       "      <td>TRAIN9995.png</td>\n",
       "      <td>monk_9</td>\n",
       "      <td>female</td>\n",
       "      <td>18_30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12279</th>\n",
       "      <td>TRAIN9996.png</td>\n",
       "      <td>monk_4</td>\n",
       "      <td>female</td>\n",
       "      <td>18_30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12281</th>\n",
       "      <td>TRAIN9998.png</td>\n",
       "      <td>monk_5</td>\n",
       "      <td>male</td>\n",
       "      <td>18_30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12282</th>\n",
       "      <td>TRAIN9999.png</td>\n",
       "      <td>monk_4</td>\n",
       "      <td>male</td>\n",
       "      <td>18_30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8553 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                name skin_tone  gender    age\n",
       "1      TRAIN0001.png    monk_1  female  18_30\n",
       "2      TRAIN0002.png    monk_6    male   0_17\n",
       "4      TRAIN0004.png    monk_3    male   0_17\n",
       "5      TRAIN0005.png    monk_2    male   0_17\n",
       "6      TRAIN0006.png    monk_2  female  18_30\n",
       "...              ...       ...     ...    ...\n",
       "12276  TRAIN9993.png    monk_2    male  18_30\n",
       "12278  TRAIN9995.png    monk_9  female  18_30\n",
       "12279  TRAIN9996.png    monk_4  female  18_30\n",
       "12281  TRAIN9998.png    monk_5    male  18_30\n",
       "12282  TRAIN9999.png    monk_4    male  18_30\n",
       "\n",
       "[8553 rows x 4 columns]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "4e15050c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-19 12:13:37,227 - INFO - Upsampling skin_tone\n",
      "2022-11-19 12:13:37,231 - INFO - Sampling 0 images for skin_tone monk_3\n",
      "2022-11-19 12:13:37,233 - INFO - Sampling 148 images for skin_tone monk_4\n",
      "2022-11-19 12:13:39,538 - INFO - Sampling 415 images for skin_tone monk_5\n",
      "2022-11-19 12:13:47,298 - INFO - Sampling 417 images for skin_tone monk_2\n",
      "2022-11-19 12:13:56,334 - INFO - Sampling 1006 images for skin_tone monk_7\n",
      "2022-11-19 12:14:13,485 - INFO - Sampling 1068 images for skin_tone monk_6\n",
      "2022-11-19 12:14:34,640 - INFO - Sampling 1185 images for skin_tone monk_1\n",
      "2022-11-19 12:14:56,576 - INFO - Sampling 1215 images for skin_tone monk_8\n",
      "2022-11-19 12:15:18,870 - INFO - Sampling 1411 images for skin_tone monk_9\n",
      "2022-11-19 12:15:29,875 - INFO - Sampling 1582 images for skin_tone monk_10\n",
      "2022-11-19 12:15:35,872 - INFO - Upsampling gender\n",
      "2022-11-19 12:15:35,879 - INFO - Sampling 0 images for gender female\n",
      "2022-11-19 12:15:35,880 - INFO - Sampling 1659 images for gender male\n",
      "2022-11-19 12:16:22,689 - INFO - Upsampling age\n",
      "2022-11-19 12:16:22,696 - INFO - Sampling 0 images for age 18_30\n",
      "2022-11-19 12:16:22,698 - INFO - Sampling 2124 images for age 31_60\n",
      "2022-11-19 12:17:11,103 - INFO - Sampling 2349 images for age 0_17\n",
      "2022-11-19 12:18:02,646 - INFO - Sampling 3974 images for age 61_100\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "def upsample_imgs(df: pd.DataFrame, sample_cols: List[str], seq: iaa.Sequential) -> pd.DataFrame:\n",
    "    aug_df = pd.DataFrame()\n",
    "    for col in sample_cols:\n",
    "        logging.info(f\"Upsampling {col}\")\n",
    "        counts = df[col].value_counts()\n",
    "        img_to_sample = counts.max() - counts\n",
    "        aug_df = pd.concat([aug_df, create_augs(df, img_to_sample, seq=seq, colname=col)])\n",
    "    return aug_df\n",
    "\n",
    "aug_df = upsample_imgs(df_labeled, df_labeled.columns[1:].tolist(), seq=newseq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "2ff9c059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "monk_6     2158\n",
      "monk_7     1977\n",
      "monk_3     1934\n",
      "monk_5     1905\n",
      "monk_2     1862\n",
      "monk_8     1825\n",
      "monk_10    1760\n",
      "monk_9     1747\n",
      "monk_1     1710\n",
      "monk_4     1675\n",
      "Name: skin_tone, dtype: int64\n",
      "male      10115\n",
      "female     8438\n",
      "Name: gender, dtype: int64\n",
      "0_17      4928\n",
      "31_60     4699\n",
      "18_30     4554\n",
      "61_100    4372\n",
      "Name: age, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "for col in df_labeled.columns[1:]:\n",
    "    print(aug_df[col].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf14ba8",
   "metadata": {},
   "source": [
    "## Specify the Model\n",
    "\n",
    "We define a single model class that is able train on the data in `X` and `Y` and predict outcomes for all three classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "40c55abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictionModel():\n",
    "    def __init__(self, X, Y, idx):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.idx = idx\n",
    "        self.trainX, self.testX = X[idx[0],:], X[idx[1],:]\n",
    "        self.trainY, self.testY = [Y[i][idx[0],:] for i in range(3)], [Y[i][idx[1],:] for i in range(3)]\n",
    "        self.cat = ['skin_tone','gender','age']\n",
    "        self.loss = ['categorical_crossentropy' for i in range(3)]\n",
    "        self.metrics = [['accuracy'] for i in range(3)]\n",
    "        self.models = [None]*3\n",
    "\n",
    "    # train a model specific for a certain class index in self.cat\n",
    "    def fit(self, index, model, epochs=5, batch_size=32, save=False, save_location=None, verbose=1):\n",
    "        \n",
    "        if verbose: print(\"Training model for \"+self.cat[index])\n",
    "        model.add(K.layers.Dense(self.trainY[index].shape[1], activation='softmax'))\n",
    "        model.compile(loss=self.loss[index], optimizer='Adam', metrics=self.metrics[index])\n",
    "        model.fit(\n",
    "            self.trainX, self.trainY[index], \n",
    "            validation_data=(self.testX,self.testY[index]), \n",
    "            batch_size=batch_size, epochs=epochs, verbose=verbose\n",
    "        )\n",
    "        if save:\n",
    "            if os.path.exists(SAVEPATH)==False:\n",
    "                print('save location '+SAVEPATH+' did not exist. creating')\n",
    "                os.makedirs(SAVEPATH)\n",
    "            SAVE_LOCATION = save_location+'model_'+cat[index]+'.h5'\n",
    "            print(\"saving model at \"+SAVE_LOCATION)\n",
    "            model.save(SAVE_LOCATION)\n",
    "        self.models[index] = model\n",
    "            \n",
    "    def predict(self, newX):\n",
    "        predictions = [model.predict(newX) for model in self.models]\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76ae9ef",
   "metadata": {},
   "source": [
    "## Initialize and Train a Model\n",
    "\n",
    "We now train a `PredictionModel` to predict the likely skin tone, gender, and age of an input image. This baseline model is initialize on imagenet weights and uses the ResNet50 architecture. We strongly recommend using a GPU to reduce training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "da29fedf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for skin_tone\n",
      "Epoch 1/5\n",
      "188/188 [==============================] - 16s 85ms/step - loss: 2.6600 - accuracy: 0.1320 - val_loss: 4.7888 - val_accuracy: 0.1360\n",
      "Epoch 2/5\n",
      "188/188 [==============================] - 14s 77ms/step - loss: 2.3338 - accuracy: 0.1710 - val_loss: 2.1001 - val_accuracy: 0.2062\n",
      "Epoch 3/5\n",
      "188/188 [==============================] - 15s 77ms/step - loss: 2.1921 - accuracy: 0.1862 - val_loss: 2.0502 - val_accuracy: 0.2241\n",
      "Epoch 4/5\n",
      "188/188 [==============================] - 15s 77ms/step - loss: 2.0914 - accuracy: 0.2009 - val_loss: 1.9529 - val_accuracy: 0.2249\n",
      "Epoch 5/5\n",
      "188/188 [==============================] - 14s 77ms/step - loss: 2.0117 - accuracy: 0.2220 - val_loss: 1.9467 - val_accuracy: 0.2284\n",
      "save location ./models/ did not exist. creating\n",
      "saving model at ./models/model_skin_tone.h5\n",
      "Training model for gender\n",
      "Epoch 1/5\n",
      "188/188 [==============================] - 16s 85ms/step - loss: 0.7269 - accuracy: 0.6130 - val_loss: 0.6789 - val_accuracy: 0.7330\n",
      "Epoch 2/5\n",
      "188/188 [==============================] - 14s 77ms/step - loss: 0.5473 - accuracy: 0.7099 - val_loss: 0.4999 - val_accuracy: 0.7432\n",
      "Epoch 3/5\n",
      "188/188 [==============================] - 14s 77ms/step - loss: 0.4957 - accuracy: 0.7419 - val_loss: 0.5029 - val_accuracy: 0.7553\n",
      "Epoch 4/5\n",
      "188/188 [==============================] - 14s 77ms/step - loss: 0.4427 - accuracy: 0.7718 - val_loss: 0.4637 - val_accuracy: 0.7681\n",
      "Epoch 5/5\n",
      "188/188 [==============================] - 15s 77ms/step - loss: 0.4198 - accuracy: 0.7885 - val_loss: 0.5197 - val_accuracy: 0.7736\n",
      "saving model at ./models/model_gender.h5\n",
      "Training model for age\n",
      "Epoch 1/5\n",
      "188/188 [==============================] - 16s 85ms/step - loss: 1.4974 - accuracy: 0.3663 - val_loss: 1.3282 - val_accuracy: 0.5413\n",
      "Epoch 2/5\n",
      "188/188 [==============================] - 15s 79ms/step - loss: 1.2276 - accuracy: 0.4732 - val_loss: 1.0369 - val_accuracy: 0.5401\n",
      "Epoch 3/5\n",
      "188/188 [==============================] - 14s 77ms/step - loss: 1.1098 - accuracy: 0.5155 - val_loss: 0.9914 - val_accuracy: 0.5577\n",
      "Epoch 4/5\n",
      "188/188 [==============================] - 15s 78ms/step - loss: 1.0553 - accuracy: 0.5469 - val_loss: 0.9747 - val_accuracy: 0.5705\n",
      "Epoch 5/5\n",
      "188/188 [==============================] - 15s 78ms/step - loss: 0.9993 - accuracy: 0.5727 - val_loss: 0.9679 - val_accuracy: 0.5752\n",
      "saving model at ./models/model_age.h5\n"
     ]
    }
   ],
   "source": [
    "# function to initialize a model\n",
    "def initializeModel():\n",
    "    res_model = ResNet50(include_top=False, weights='imagenet', input_tensor=K.Input(shape=[length,width,3]))\n",
    "\n",
    "    # freeze all but the last layer\n",
    "    for layer in res_model.layers[:143]:\n",
    "        layer.trainable = False\n",
    "    model = K.models.Sequential()\n",
    "    model.add(res_model)\n",
    "    model.add(K.layers.Flatten())\n",
    "    model.add(K.layers.BatchNormalization())\n",
    "    model.add(K.layers.Dense(256, activation='relu'))\n",
    "    model.add(K.layers.Dropout(0.5))\n",
    "    model.add(K.layers.BatchNormalization())\n",
    "    model.add(K.layers.Dense(128, activation='relu'))\n",
    "    model.add(K.layers.Dropout(0.5))\n",
    "    model.add(K.layers.BatchNormalization())\n",
    "    model.add(K.layers.Dense(64, activation='relu'))\n",
    "    model.add(K.layers.Dropout(0.5))\n",
    "    model.add(K.layers.BatchNormalization())\n",
    "    return model\n",
    "\n",
    "nntrain = int(0.7*nn)\n",
    "np.random.seed(42)\n",
    "indices = np.random.permutation(nn)\n",
    "train_idx, test_idx = indices[:nntrain], indices[nntrain:]\n",
    "mymodel = PredictionModel(X=X, Y=Y, idx=[train_idx,test_idx])\n",
    "\n",
    "# train model\n",
    "for i in range(3):\n",
    "    mymodel.fit(index=i, model=initializeModel(), epochs=5, save=True, save_location=SAVEPATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6527b5ae",
   "metadata": {},
   "source": [
    "## Evaluate the Model\n",
    "\n",
    "We now evaluate the model on the test data. To do this, let's first load up that data and structure it similarly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "43ab27c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting test labels to np array\n",
      "Loading test images\n",
      "Converting test images to np array\n"
     ]
    }
   ],
   "source": [
    "# load labels data\n",
    "TESTPATH = './test/'\n",
    "df_test = pd.read_csv(TESTPATH+'labels.csv')\n",
    "\n",
    "# Convert labels to np array\n",
    "print(\"Converting test labels to np array\")\n",
    "testY = []\n",
    "for i in range(3):\n",
    "    lab = lbs[i].fit_transform(df_test[cat[i]])\n",
    "    if lab.shape[1]==1:\n",
    "        testY.append(np.hstack((1-lab,lab)))\n",
    "    else:\n",
    "        testY.append(lab)\n",
    "        \n",
    "# load and convert images into np array\n",
    "print(\"Loading test images\")\n",
    "nt = df_test.shape[0]\n",
    "all_imgs = [image.load_img(TESTPATH+df_test.iloc[i]['name'], target_size=(length,width)) for i in range(nt)]\n",
    "\n",
    "print(\"Converting test images to np array\")\n",
    "testX = np.empty([nt, length, width, 3], dtype=float)\n",
    "for i in range(nt):\n",
    "    testX[i,:] = image.img_to_array(all_imgs[i])\n",
    "testX = K.applications.resnet50.preprocess_input(testX)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07ee94e",
   "metadata": {},
   "source": [
    "We then obtain predicted labels for skin tone, gender, and age as a list of lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bf106302",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = mymodel.predict(testX)\n",
    "predY = [[np.argmax(pred[i][j,:]) for j in range(nt)] for i in range(3)]\n",
    "predLabels = [[lbs[i].classes_[j] for j in predY[i]] for i in range(3)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0cf73d",
   "metadata": {},
   "source": [
    "Finally, we calculate the label-wise accuracy and disparity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "081d5cdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': {'skin_tone': 0.25766666666666665,\n",
       "  'gender': 0.7966666666666666,\n",
       "  'age': 0.5783333333333334},\n",
       " 'disparity': {'skin_tone': 0.5514223194748359,\n",
       "  'gender': 0.1493182689960596,\n",
       "  'age': 0.7802908824936}}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate accuracy\n",
    "acc = {}\n",
    "for i in range(3):\n",
    "    icat = cat[i]\n",
    "    iacc = accuracy_score(df_test[cat[i]], predLabels[i])\n",
    "    acc[icat] = iacc\n",
    "\n",
    "# calculate disparity\n",
    "def disparity_score(ytrue, ypred):\n",
    "    cm = confusion_matrix(ytrue,ypred)\n",
    "    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    all_acc = list(cm.diagonal())\n",
    "    return max(all_acc) - min(all_acc)\n",
    "\n",
    "disp = {}\n",
    "for i in range(3):\n",
    "    icat = cat[i]\n",
    "    idisp = disparity_score(df_test[cat[i]], predLabels[i])\n",
    "    disp[icat] = idisp\n",
    "disp\n",
    "\n",
    "results = {'accuracy': acc, 'disparity': disp}\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20b0fcd",
   "metadata": {},
   "source": [
    "# Score Model and Prepare Submission\n",
    "\n",
    "Based on the above metric, we now calculate the score to evaluate your submission. This score will be displayed in your public leaderboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "db5b9086",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'submission_name': '8-Bit Bias Bounty Baseline',\n",
       " 'score': 4.705572543130653,\n",
       " 'metrics': {'accuracy': {'skin_tone': 0.25766666666666665,\n",
       "   'gender': 0.7966666666666666,\n",
       "   'age': 0.5783333333333334},\n",
       "  'disparity': {'skin_tone': 0.5514223194748359,\n",
       "   'gender': 0.1493182689960596,\n",
       "   'age': 0.7802908824936}}}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getScore(results):\n",
    "    acc = results['accuracy']\n",
    "    disp = results['disparity']\n",
    "    ad = 2*acc['gender']*(1-disp['gender']) + 4*acc['age']*(1-disp['age']**2) + 10*acc['skin_tone']*(1-disp['skin_tone']**5)\n",
    "    return ad\n",
    "\n",
    "title = '8-Bit Bias Bounty Baseline'\n",
    "    \n",
    "submission = {\n",
    "    'submission_name': title,\n",
    "    'score': getScore(results),\n",
    "    'metrics': results\n",
    "}\n",
    "submission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19070a4e",
   "metadata": {},
   "source": [
    "Finally, let's export this as a json file to upload as part of filling out your [submission form](https://docs.google.com/forms/d/e/1FAIpQLSfwqtVkJBVRP6TnFp7vHbbH8SlwKZJFIjvGQy7TyYFc8HR1hw/viewform)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34c30e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"baseline_score.json\", \"w\") as f:\n",
    "    json.dump(submission, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.11 ('cds-vis')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "83db3168067e97115b16528dd771939296c5a9869e87b43a6cc607e3277c1e98"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
